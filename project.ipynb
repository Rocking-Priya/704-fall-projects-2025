{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocking-Priya/704-fall-projects-2025/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "5b822836-3230-484f-897b-1f3e8d50d57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIAL SAMPLE 2047\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "f056278d-09d9-41d9-ace7-6898bd14788d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CURR STATE 1923 ACTION 1 NEXT REWARD 0 NEXT STATE 1804\n",
            "CURR STATE 1804 ACTION 1 NEXT REWARD 0 NEXT STATE 1677\n",
            "CURR STATE 1677 ACTION 0 NEXT REWARD 0 NEXT STATE 1541\n",
            "CURR STATE 1541 ACTION 0 NEXT REWARD 0 NEXT STATE 1413\n",
            "CURR STATE 1413 ACTION 0 NEXT REWARD 0 NEXT STATE 1285\n",
            "CURR STATE 1285 ACTION 1 NEXT REWARD 0 NEXT STATE 1158\n",
            "CURR STATE 1158 ACTION -1 NEXT REWARD 0 NEXT STATE 1157\n",
            "CURR STATE 1157 ACTION -1 NEXT REWARD 0 NEXT STATE 1028\n",
            "CURR STATE 1028 ACTION 1 NEXT REWARD 0 NEXT STATE 901\n",
            "CURR STATE 901 ACTION -1 NEXT REWARD 0 NEXT STATE 772\n",
            "CURR STATE 772 ACTION 0 NEXT REWARD 0 NEXT STATE 644\n",
            "CURR STATE 644 ACTION -1 NEXT REWARD 0 NEXT STATE 515\n",
            "CURR STATE 515 ACTION -1 NEXT REWARD 0 NEXT STATE 394\n",
            "CURR STATE 394 ACTION 0 NEXT REWARD 0 NEXT STATE 402\n",
            "CURR STATE 402 ACTION 0 NEXT REWARD 0 NEXT STATE 410\n",
            "CURR STATE 410 ACTION -1 NEXT REWARD 0 NEXT STATE 417\n",
            "CURR STATE 417 ACTION -1 NEXT REWARD 0 NEXT STATE 552\n",
            "CURR STATE 552 ACTION 1 NEXT REWARD 0 NEXT STATE 681\n",
            "CURR STATE 681 ACTION 1 NEXT REWARD 0 NEXT STATE 818\n",
            "CURR STATE 818 ACTION 1 NEXT REWARD 0 NEXT STATE 827\n",
            "CURR STATE 827 ACTION -1 NEXT REWARD 0 NEXT STATE 706\n",
            "CURR STATE 706 ACTION 0 NEXT REWARD 0 NEXT STATE 714\n",
            "CURR STATE 714 ACTION -1 NEXT REWARD 0 NEXT STATE 721\n",
            "CURR STATE 721 ACTION 1 NEXT REWARD 0 NEXT STATE 858\n",
            "CURR STATE 858 ACTION 0 NEXT REWARD 0 NEXT STATE 866\n",
            "CURR STATE 866 ACTION 1 NEXT REWARD 0 NEXT STATE 875\n",
            "CURR STATE 875 ACTION 1 NEXT REWARD 0 NEXT STATE 756\n",
            "CURR STATE 756 ACTION 0 NEXT REWARD 0 NEXT STATE 628\n",
            "CURR STATE 628 ACTION 0 NEXT REWARD 0 NEXT STATE 500\n",
            "CURR STATE 500 ACTION 1 NEXT REWARD 0 NEXT STATE 373\n",
            "CURR STATE 373 ACTION 0 NEXT REWARD 0 NEXT STATE 237\n",
            "CURR STATE 237 ACTION -1 NEXT REWARD 0 NEXT STATE 100\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"log-random.tsv\", \"w\") as f:\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\n\")\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\n\")\n",
        ""
      ],
      "metadata": {
        "id": "D14Rr0DVX4v0"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab.files\n",
        "\n",
        "google.colab.files.download(\"log-random.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QI58fNKfYBTr",
        "outputId": "da7fe43e-72be-4990-e5e8-51b2943d41d4"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_68fd73c5-a641-42bb-bb28-4c60b51cca9e\", \"log-random.tsv\", 504)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "num_episodes = 32\n",
        "max_steps = 1024\n",
        "Q = {}                # dictionary: key=(state,action) -> float value\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "with open(\"q-random.tsv\", \"w\") as f:\n",
        "    # Write the header **once**\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\told_value\\tnew_value\\n\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=max_steps):\n",
        "\n",
        "            # old Q-value\n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "\n",
        "            # best next Q-value\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            best_next = max(Q.get((next_state, a), 0.0) for a in next_actions)\n",
        "\n",
        "            # Q-learning update (alpha=1 → full replacement)\n",
        "            new_value = old_value + alpha + (next_reward + gamma * best_next - old_value)\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "            # write the data row\n",
        "            f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\t{old_value}\\t{new_value}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"q-random.tsv\", sep=\"\\t\")\n",
        "\n",
        "print(\"Data rows:\", len(df))  # should be 32768\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6m6X9kTqmSA",
        "outputId": "fafd20f9-6a70-4685-ae4b-cca9a51373d5"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data rows: 32768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"q-random.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BjbeUwVqaoNT",
        "outputId": "cb385c7d-5f76-4c62-8fdb-3b49f42160a3"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_511c1943-66a4-483a-8c48-b8ccbe66a896\", \"q-random.tsv\", 1341668)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "# Build start states (exactly 32 and including 64)\n",
        "all_inits = simulator.initial_states[:]   # length 71\n",
        "if 64 not in all_inits:\n",
        "    all_inits.append(64)\n",
        "other_states = [s for s in all_inits if s != 64]\n",
        "start_states = other_states[:31] + [64]   # length 32, includes 64\n",
        "\n",
        "Q = {}               # empty Q-table: key = (state, action) -> value\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.25\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    # With probability epsilon, pick a random action\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    # Otherwise pick the best action according to current Q-values\n",
        "    q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "    max_q = max(q_values)\n",
        "    best_actions = [a for a, qv in zip(actions, q_values) if qv == max_q]\n",
        "    return random.choice(best_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "max_steps = 1024\n",
        "\n",
        "with open(\"q-greedy.tsv\", \"w\") as f:\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\told_value\\tnew_value\\n\")\n",
        "\n",
        "    for init_state in start_states:\n",
        "        curr_state = init_state\n",
        "        for step in range(max_steps):\n",
        "            curr_action = epsilon_greedy_policy(curr_state, simulator.get_actions(curr_state))\n",
        "            next_reward, next_state = simulator.get_next_reward_state(curr_state, curr_action)\n",
        "\n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            best_next = max(Q.get((next_state, a), 0.0) for a in next_actions)\n",
        "            new_value = old_value + alpha * (next_reward + gamma * best_next - old_value)\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "            f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\t{old_value}\\t{new_value}\\n\")\n",
        "\n",
        "            curr_state = next_state\n"
      ],
      "metadata": {
        "id": "SE8WXKrfHw7I"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"q-greedy.tsv\", sep=\"\\t\")\n",
        "print(\"Data rows:\", len(df))  # should be 32768\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRKDZg85tu6d",
        "outputId": "04d8e101-c93c-4867-8380-b1259c42b6ff"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data rows: 32768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"q-greedy.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oSImFzpQjgYv",
        "outputId": "c8e5c939-4b13-4bee-bd7d-25bcdebe3dde"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_effd6bbf-debd-4115-97e6-355d336990d0\", \"q-greedy.tsv\", 690931)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "w7VnSBcYDINb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "# optional: reproducible tie-breaking\n",
        "random.seed(0)\n",
        "\n",
        "# 1) Read q-greedy log (chronological rows)\n",
        "df = pd.read_csv(\"q-greedy.tsv\", sep=\"\\t\")\n",
        "\n",
        "# 2) Compute final Q-values for each (state,action) by taking the last new_value\n",
        "grouped = df.groupby(['curr_state', 'curr_action'], sort=False, as_index=False).last()\n",
        "\n",
        "Q_final = {}\n",
        "for _, row in grouped.iterrows():\n",
        "    s = int(row['curr_state'])\n",
        "    a = int(row['curr_action'])\n",
        "    v = float(row['new_value'])\n",
        "    Q_final[(s, a)] = v\n",
        "\n",
        "# 3) Set of states we want policies for: all unique curr_state values in the q-log\n",
        "states = sorted(int(x) for x in df['curr_state'].unique())\n",
        "\n",
        "# 4) Create policy file covering every state in `states`\n",
        "with open(\"policy-greedy.tsv\", \"w\") as f:\n",
        "    f.write(\"state\\taction\\n\")\n",
        "    for s in states:\n",
        "        actions = simulator.get_actions(s)   # [-1,0,1]\n",
        "        q_values = [Q_final.get((s, a), 0.0) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        # tie-break randomly among best actions\n",
        "        best_actions = [a for a, qv in zip(actions, q_values) if qv == max_q]\n",
        "        chosen = random.choice(best_actions)\n",
        "        f.write(f\"{s}\\t{int(chosen)}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_policy = pd.read_csv(\"policy-greedy.tsv\", sep=\"\\t\")\n",
        "print(\"Number of states in policy:\", len(df_policy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHtZlEJmxhCp",
        "outputId": "61d3c723-cd97-4ccf-8456-48882a411d5d"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states in policy: 1970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_q = pd.read_csv(\"q-greedy.tsv\", sep=\"\\t\")\n",
        "df_policy = pd.read_csv(\"policy-greedy.tsv\", sep=\"\\t\")\n",
        "\n",
        "states_in_q = set(df_q['curr_state'].unique())\n",
        "states_in_policy = set(df_policy['state'].unique())\n",
        "\n",
        "print(\"states_in_q count:\", len(states_in_q))\n",
        "print(\"states_in_policy count:\", len(states_in_policy))\n",
        "print(\"Missing in policy (should be empty):\", sorted(list(states_in_q - states_in_policy))[:10])\n",
        "print(\"Extra in policy (should be empty):\", sorted(list(states_in_policy - states_in_q))[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhnNlloEgsk9",
        "outputId": "9a91fda4-4744-4fdb-94d4-ef8bcd0d02f5"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "states_in_q count: 1970\n",
            "states_in_policy count: 1970\n",
            "Missing in policy (should be empty): []\n",
            "Extra in policy (should be empty): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"policy-greedy.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BD2e_dkGkLaO",
        "outputId": "2c8628dc-d384-431a-ec6a-4b02fef6aa4f"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1c8af993-e4bb-4d8d-a3e8-ba45b801f23e\", \"policy-greedy.tsv\", 13416)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "b1A9W4gCDiRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e260d4-6a0f-4546-d2ed-05b560678eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing initial states in policy (should be empty): []\n",
            "Policy actions leading outside policy states (should be empty): []\n",
            "policy size (states): 1993\n",
            "unique Q states: 1993\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "\n",
        "random.seed(0)           # optional reproducibility\n",
        "\n",
        "# Parameters\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.25          # 25% random actions in epsilon-greedy\n",
        "episodes_per_start = 25  # how many episodes from each initial state (tune this)\n",
        "max_steps = 1024\n",
        "\n",
        "# Prepare start states (all simulator.initial_states)\n",
        "start_states = list(simulator.initial_states)  # 71 initial states\n",
        "# Optionally ensure center/64 is included (you already did earlier)\n",
        "if simulator.terminal_state not in start_states:\n",
        "    pass  # terminal isn't an initial state; we don't start there\n",
        "\n",
        "# Q table\n",
        "Q = {}   # key: (state, action) -> float\n",
        "\n",
        "def epsilon_greedy_action(state, actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "    max_q = max(q_values)\n",
        "    best_actions = [a for a, qv in zip(actions, q_values) if qv == max_q]\n",
        "    return random.choice(best_actions)\n",
        "\n",
        "# Train: for every start state, run many short episodes; break after reward\n",
        "# Log Q updates optionally to a file (q-optimal.tsv)\n",
        "with open(\"q-optimal.tsv\", \"w\") as fout:\n",
        "    fout.write(\"episode\\tstep\\tcurr_state\\tcurr_action\\tnext_reward\\tnext_state\\told_value\\tnew_value\\n\")\n",
        "    episode_id = 0\n",
        "    for start_state in start_states:\n",
        "        for ep in range(episodes_per_start):\n",
        "            episode_id += 1\n",
        "            curr_state = start_state\n",
        "            for step in range(max_steps):\n",
        "                curr_action = epsilon_greedy_action(curr_state, simulator.get_actions(curr_state))\n",
        "                next_reward, next_state = simulator.get_next_reward_state(curr_state, curr_action)\n",
        "\n",
        "                old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "                next_actions = simulator.get_actions(next_state)\n",
        "                best_next = max(Q.get((next_state, a), 0.0) for a in next_actions)\n",
        "\n",
        "                # general Q-update (works for any alpha)\n",
        "                new_value = old_value + alpha * (next_reward + gamma * best_next - old_value)\n",
        "                Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "                fout.write(f\"{episode_id}\\t{step}\\t{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\t{old_value}\\t{new_value}\\n\")\n",
        "\n",
        "                curr_state = next_state\n",
        "\n",
        "                # stop episode after the non-zero reward has been observed and updated\n",
        "                if next_reward != 0:\n",
        "                    break\n",
        "\n",
        "# After training, extract policy from Q\n",
        "# Build Q_final (final new_value per (state,action)) — Q already contains final values\n",
        "Q_final = dict(Q)  # shallow copy\n",
        "\n",
        "# States to produce a policy for: all states seen as curr_state in training\n",
        "states = sorted({s for (s, a) in Q_final.keys()})\n",
        "\n",
        "# For safety, also compute set of visited next_states (so we can avoid actions leading to unknown states)\n",
        "visited_states = set(states)  # states where we have Q entries as curr_state\n",
        "\n",
        "# Create policy-optimal.tsv ensuring actions do not lead to unknown states\n",
        "with open(\"policy-optimal.tsv\", \"w\") as f:\n",
        "    f.write(\"state\\taction\\n\")\n",
        "    for s in states:\n",
        "        actions = simulator.get_actions(s)\n",
        "        # prefer actions that are known and lead to visited states\n",
        "        candidate_actions = []\n",
        "        for a in actions:\n",
        "            # compute next_state for this action deterministically\n",
        "            _, predicted_next = simulator.get_next_reward_state(s, a)\n",
        "            if predicted_next in visited_states:\n",
        "                candidate_actions.append(a)\n",
        "        # If no candidate action leads to a visited state, fallback to all actions\n",
        "        candidate_list = candidate_actions if candidate_actions else actions\n",
        "\n",
        "        q_values = [Q_final.get((s, a), 0.0) for a in candidate_list]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, qv in zip(candidate_list, q_values) if qv == max_q]\n",
        "        chosen = random.choice(best_actions)\n",
        "        f.write(f\"{s}\\t{int(chosen)}\\n\")\n",
        "\n",
        "# Validation checks:\n",
        "df_policy = pd.read_csv(\"policy-optimal.tsv\", sep=\"\\t\")\n",
        "policy_states = set(df_policy['state'].unique())\n",
        "\n",
        "# 1) Coverage of all initial states\n",
        "missing_initials = [s for s in simulator.initial_states if s not in policy_states]\n",
        "print(\"Missing initial states in policy (should be empty):\", missing_initials)\n",
        "\n",
        "# 2) Policy actions don't lead to unknown states\n",
        "bad_actions = []\n",
        "for _, row in df_policy.iterrows():\n",
        "    s = int(row['state'])\n",
        "    a = int(row['action'])\n",
        "    _, next_s = simulator.get_next_reward_state(s, a)\n",
        "    if next_s not in policy_states:\n",
        "        bad_actions.append((s, a, next_s))\n",
        "print(\"Policy actions leading outside policy states (should be empty):\", bad_actions[:10])\n",
        "\n",
        "# 3) Basic counts\n",
        "print(\"policy size (states):\", len(policy_states))\n",
        "print(\"unique Q states:\", len(states))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"policy-optimal.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5Uf_heNlk4uF",
        "outputId": "a985a383-1238-4374-c70c-7acebc65996b"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9e05c5e8-74c8-4682-8995-c82a64a60e43\", \"policy-optimal.tsv\", 13658)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}