{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rocking-Priya/704-fall-projects-2025/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 8 Project\n",
        "This homework will modify a simulator controlling a small vehicle to implement tabular q-learning.\n",
        "You will first test your code with random and greedy-epsilon policies, then tweak your own training method for a more optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvEjsVg10YFf"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 8 Materials](https://github.com/bu-cds-dx704/dx704-project-08).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7nKctadu6R"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUD8aVv44IVP"
      },
      "source": [
        "## Rover Simulator\n",
        "\n",
        "The following Python class implements a simulation of a simple vehicle with integer x,y coordinates facing in one of 8 possible directions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Sv0BRzHz187D"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "\n",
        "import random\n",
        "\n",
        "class RoverSimulator(object):\n",
        "    DIRECTIONS = ((0, 1), (1, 1), (1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1))\n",
        "\n",
        "    def __init__(self, resolution):\n",
        "        self.resolution = resolution\n",
        "        self.terminal_state = self.construct_state(resolution // 2, resolution // 2, 0)\n",
        "\n",
        "        self.initial_states = []\n",
        "        for initial_x in (0, resolution // 2, resolution - 1):\n",
        "            for initial_y in (0, resolution // 2, resolution - 1):\n",
        "                for initial_direction in range(8):\n",
        "                    initial_state = self.construct_state(initial_x, initial_y, initial_direction)\n",
        "                    if initial_state != self.terminal_state:\n",
        "                        self.initial_states.append(initial_state)\n",
        "\n",
        "    def construct_state(self, x, y, direction):\n",
        "        assert 0 <= x < self.resolution\n",
        "        assert 0 <= y < self.resolution\n",
        "        assert 0 <= direction < 8\n",
        "\n",
        "        state = (y * self.resolution + x) * 8 + direction\n",
        "        assert self.decode_state(state) == (x, y, direction)\n",
        "        return state\n",
        "\n",
        "    def decode_state(self, state):\n",
        "        direction = state % 8\n",
        "        x = (state // 8) % self.resolution\n",
        "        y = state // (8 * self.resolution)\n",
        "\n",
        "        return (x, y, direction)\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        return [-1, 0, 1]\n",
        "\n",
        "    def get_next_reward_state(self, curr_state, curr_action):\n",
        "        if curr_state == self.terminal_state:\n",
        "            # no rewards or changes from terminal state\n",
        "            return (0, curr_state)\n",
        "\n",
        "        (curr_x, curr_y, curr_direction) = self.decode_state(curr_state)\n",
        "        (curr_dx, curr_dy) = self.DIRECTIONS[curr_direction]\n",
        "\n",
        "        assert self.construct_state(curr_x, curr_y, curr_direction) == curr_state\n",
        "\n",
        "        assert curr_action in (-1, 0, 1)\n",
        "\n",
        "        next_x = min(max(0, curr_x + curr_dx), self.resolution - 1)\n",
        "        next_y = min(max(0, curr_y + curr_dy), self.resolution - 1)\n",
        "        next_direction = (curr_direction + curr_action) % 8\n",
        "\n",
        "        next_state = self.construct_state(next_x, next_y, next_direction)\n",
        "        next_reward = 1 if next_state == self.terminal_state else 0\n",
        "\n",
        "        return (next_reward, next_state)\n",
        "\n",
        "    def rollout_policy(self, policy_func, max_steps=1000):\n",
        "        curr_state = self.sample_initial_state()\n",
        "        for i in range(max_steps):\n",
        "            curr_action = policy_func(curr_state, self.get_actions(curr_state))\n",
        "            (next_reward, next_state) = self.get_next_reward_state(curr_state, curr_action)\n",
        "            yield (curr_state, curr_action, next_reward, next_state)\n",
        "            curr_state = next_state\n",
        "\n",
        "    def sample_initial_state(self):\n",
        "        return random.choice(self.initial_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LMQrlfX4Ybs",
        "outputId": "8929d7a5-46d9-412c-efc0-4243cc0e1039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INITIAL SAMPLE 65\n"
          ]
        }
      ],
      "source": [
        "simulator = RoverSimulator(16)\n",
        "initial_sample = simulator.sample_initial_state()\n",
        "print(\"INITIAL SAMPLE\", initial_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Implement a Random Policy\n",
        "\n",
        "Random policies are often used to test simulators and start initial exploration.\n",
        "Implement a random policy for these simulators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DewHlicn4PtW"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "def random_policy(state, actions):\n",
        "    return random.choice(actions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJYOB9zl6szl"
      },
      "source": [
        "Use the code below to test your random policy.\n",
        "Then modify it to save the results in \"log-random.tsv\" with the columns curr_state, curr_action, next_reward and next_state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgnNCJH453qE",
        "outputId": "52ab4f5e-4352-42a6-8879-f04dfdbeaa58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CURR STATE 123 ACTION 1 NEXT REWARD 0 NEXT STATE 124\n",
            "CURR STATE 124 ACTION -1 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION 0 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION 1 NEXT REWARD 0 NEXT STATE 124\n",
            "CURR STATE 124 ACTION 1 NEXT REWARD 0 NEXT STATE 125\n",
            "CURR STATE 125 ACTION -1 NEXT REWARD 0 NEXT STATE 116\n",
            "CURR STATE 116 ACTION -1 NEXT REWARD 0 NEXT STATE 115\n",
            "CURR STATE 115 ACTION -1 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION 1 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION 0 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION 0 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION -1 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION 1 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION -1 NEXT REWARD 0 NEXT STATE 122\n",
            "CURR STATE 122 ACTION 1 NEXT REWARD 0 NEXT STATE 123\n",
            "CURR STATE 123 ACTION 1 NEXT REWARD 0 NEXT STATE 124\n",
            "CURR STATE 124 ACTION 0 NEXT REWARD 0 NEXT STATE 124\n",
            "CURR STATE 124 ACTION 1 NEXT REWARD 0 NEXT STATE 125\n",
            "CURR STATE 125 ACTION -1 NEXT REWARD 0 NEXT STATE 116\n",
            "CURR STATE 116 ACTION 1 NEXT REWARD 0 NEXT STATE 117\n",
            "CURR STATE 117 ACTION 1 NEXT REWARD 0 NEXT STATE 110\n",
            "CURR STATE 110 ACTION 1 NEXT REWARD 0 NEXT STATE 103\n",
            "CURR STATE 103 ACTION -1 NEXT REWARD 0 NEXT STATE 222\n",
            "CURR STATE 222 ACTION 1 NEXT REWARD 0 NEXT STATE 215\n",
            "CURR STATE 215 ACTION 1 NEXT REWARD 0 NEXT STATE 328\n",
            "CURR STATE 328 ACTION -1 NEXT REWARD 0 NEXT STATE 463\n",
            "CURR STATE 463 ACTION 1 NEXT REWARD 0 NEXT STATE 576\n",
            "CURR STATE 576 ACTION 0 NEXT REWARD 0 NEXT STATE 704\n",
            "CURR STATE 704 ACTION -1 NEXT REWARD 0 NEXT STATE 839\n",
            "CURR STATE 839 ACTION -1 NEXT REWARD 0 NEXT STATE 958\n",
            "CURR STATE 958 ACTION -1 NEXT REWARD 0 NEXT STATE 949\n",
            "CURR STATE 949 ACTION -1 NEXT REWARD 0 NEXT STATE 812\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "    print(\"CURR STATE\", curr_state, \"ACTION\", curr_action, \"NEXT REWARD\", next_reward, \"NEXT STATE\", next_state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"log-random.tsv\", \"w\") as f:\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\n\")\n",
        "    for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=32):\n",
        "        f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\n\")\n",
        ""
      ],
      "metadata": {
        "id": "D14Rr0DVX4v0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.colab.files\n",
        "\n",
        "google.colab.files.download(\"log-random.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QI58fNKfYBTr",
        "outputId": "7180fc8a-2d80-4b71-9d47-39bc95007d80"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_47163300-4382-444c-90d6-12ac61afe269\", \"log-random.tsv\", 492)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRZOd3Bk7JIz"
      },
      "source": [
        "Submit \"log-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAWky_dR7QK1"
      },
      "source": [
        "## Part 2: Implement Q-Learning with Random Policy\n",
        "\n",
        "The code below runs 32 random rollouts of 1024 steps using your random policy.\n",
        "Modify the rollout code to implement Q-Learning.\n",
        "Just implement one learning update for each sampled state-action in the simulation.\n",
        "Use $\\alpha=1$ and $\\gamma=0.9$ since the simulator is deterministic and there is a sink where the rewards stop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "231quBGA7pVd"
      },
      "outputs": [],
      "source": [
        "num_episodes = 32\n",
        "max_steps = 1024\n",
        "Q = {}                # dictionary: key=(state,action) -> float value\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "\n",
        "with open(\"q-random.tsv\", \"w\") as f:\n",
        "    # Write the header **once**\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\told_value\\tnew_value\\n\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(random_policy, max_steps=max_steps):\n",
        "\n",
        "            # old Q-value\n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "\n",
        "            # best next Q-value\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            best_next = max(Q.get((next_state, a), 0.0) for a in next_actions)\n",
        "\n",
        "            # Q-learning update (alpha=1 → full replacement)\n",
        "            new_value = next_reward + gamma * best_next\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "            # write the data row\n",
        "            f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\t{old_value}\\t{new_value}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDBNOFLcPPRs"
      },
      "source": [
        "Save each step in the simulator in a file \"q-random.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"q-random.tsv\", sep=\"\\t\")\n",
        "\n",
        "print(\"Data rows:\", len(df))  # should be 32768\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6m6X9kTqmSA",
        "outputId": "e0fb51ce-5f64-43c0-aced-3803d43622a7"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data rows: 32768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"q-random.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BjbeUwVqaoNT",
        "outputId": "8ff60bc9-45bc-4870-dc8b-3c9d26b017f0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1598b26c-f4de-4f1a-8ca8-4f82e171c7a4\", \"q-random.tsv\", 689530)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnu4j4Yp72k1"
      },
      "source": [
        "Submit \"q-random.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMBmh7UW-vJU"
      },
      "source": [
        "## Part 3: Implement Epsilon-Greedy Policy\n",
        "\n",
        "Implement an epsilon-greedy policy that picks the optimal policy based on your q-values so far 75% of the time, and picks a random action 25% of the time.\n",
        "This is a high epsilon value, but the environment is deterministic, so it will benefit from more exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "pS7g1sETAbKd"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "Q = {}               # empty Q-table: key = (state, action) -> value\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.25\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    # With probability epsilon, pick a random action\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    # Otherwise pick the best action according to current Q-values\n",
        "    q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "    max_q = max(q_values)\n",
        "    best_actions = [a for a, qv in zip(actions, q_values) if qv == max_q]\n",
        "    return random.choice(best_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSMW7CNAtEw"
      },
      "source": [
        "Combine your epsilon-greedy policy with q-learning below and save the observations and updates in \"q-greedy.tsv\" with columns curr_state, curr_action, next_reward, next_state, old_value, new_value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5nkGhMOVJFp"
      },
      "source": [
        "Hint: make sure to reset your q-learning state before running the simulation below so that the learning process is recorded from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 32\n",
        "max_steps = 1024\n",
        "\n",
        "with open(\"q-greedy.tsv\", \"w\") as f:\n",
        "    # write header\n",
        "    f.write(\"curr_state\\tcurr_action\\tnext_reward\\tnext_state\\told_value\\tnew_value\\n\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        for (curr_state, curr_action, next_reward, next_state) in simulator.rollout_policy(\n",
        "                epsilon_greedy_policy, max_steps=max_steps):\n",
        "\n",
        "            old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "            next_actions = simulator.get_actions(next_state)\n",
        "            best_next = max(Q.get((next_state, a), 0.0) for a in next_actions)\n",
        "            new_value = next_reward + gamma * best_next\n",
        "            Q[(curr_state, curr_action)] = new_value\n",
        "\n",
        "            # log step\n",
        "            f.write(f\"{curr_state}\\t{curr_action}\\t{next_reward}\\t{next_state}\\t{old_value}\\t{new_value}\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hobofXDvtjji"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vd246wcA0HV"
      },
      "source": [
        "Submit \"q-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"q-greedy.tsv\", sep=\"\\t\")\n",
        "print(\"Data rows:\", len(df))  # should be 32768\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRKDZg85tu6d",
        "outputId": "44c94be1-c0bb-42fc-ef15-4bdab14e7ffc"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data rows: 32768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"q-greedy.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oSImFzpQjgYv",
        "outputId": "95e4c8d3-4a6f-4971-eac6-77dcda0afaff"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fff225e0-1d98-41eb-9595-b3e71b024567\", \"q-greedy.tsv\", 703551)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGc8aP8DCzW"
      },
      "source": [
        "## Part 4: Extract Policy from Q-Values\n",
        "\n",
        "Using your final q-values from the previous simulation, extract a policy picking the best actions according to those q-values.\n",
        "Save the policy in a file \"policy-greedy.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "w7VnSBcYDINb"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "\n",
        "\n",
        "with open(\"policy-greedy.tsv\", \"w\") as f:\n",
        "    f.write(\"state\\taction\\n\")   # header\n",
        "\n",
        "    # Use all states observed in Q (or simulator.initial_states)\n",
        "    all_states = set(s for (s, a) in Q.keys())\n",
        "\n",
        "    for state in all_states:\n",
        "        actions = simulator.get_actions(state)\n",
        "        # pick action(s) with max Q\n",
        "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, qv in zip(actions, q_values) if qv == max_q]\n",
        "        best_action = random.choice(best_actions)   # tie-break randomly\n",
        "        f.write(f\"{state}\\t{best_action}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"policy-greedy.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BD2e_dkGkLaO",
        "outputId": "9ddaef51-e743-43ce-9429-4fe4303cc146"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_626fda44-324f-4a83-8106-9f3782841b93\", \"policy-greedy.tsv\", 13286)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLcCtb64DJl-"
      },
      "source": [
        "Submit \"policy-greedy.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1-nlr6Byq2"
      },
      "source": [
        "## Part 5: Implement Large Policy\n",
        "\n",
        "Train a more optimal policy using q-learning.\n",
        "Save the policy in a file \"policy-optimal.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHuR4N4BD3_r"
      },
      "source": [
        "Hint: this policy will be graded on its performance compared to optimal for each of the initial states.\n",
        "**You will get full credit if the average value of your policy for the initial states is within 20% of optimal.**\n",
        "Make sure that your policy has coverage of all the initial states, and does not take actions leading to states not included in your policy.\n",
        "You will have to run several rollouts to get coverage of all the initial states, and the provided loops for parts 2 and 3 only consist of one rollout each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DWSxVHTp62"
      },
      "source": [
        "Hint: this environment only gives one non-zero reward per episode, so you may want to cut off rollouts for speed once they get that reward.\n",
        "But make sure you update the q-values first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "b1A9W4gCDiRZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import random\n",
        "\n",
        "# --- Reset Q-table ---\n",
        "Q = {}\n",
        "alpha = 1.0\n",
        "gamma = 0.9\n",
        "epsilon = 0.25   # can still use epsilon-greedy for exploration\n",
        "\n",
        "# Epsilon-greedy policy for exploration\n",
        "def epsilon_greedy_policy(state, actions):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "    max_q = max(q_values)\n",
        "    best_actions = [a for a, qv in zip(actions, q_values) if qv == max_q]\n",
        "    return random.choice(best_actions)\n",
        "\n",
        "# --- Run many rollouts to cover all initial states ---\n",
        "with open(\"policy-optimal.tsv\", \"w\") as f:\n",
        "    f.write(\"state\\taction\\n\")  # header\n",
        "\n",
        "    # Repeat enough episodes to cover all initial states\n",
        "    for episode in range(200):   # increase number of episodes if needed\n",
        "        for initial_state in simulator.initial_states:\n",
        "            curr_state = initial_state\n",
        "            for step in range(1024):\n",
        "                curr_action = epsilon_greedy_policy(curr_state, simulator.get_actions(curr_state))\n",
        "                reward, next_state = simulator.get_next_reward_state(curr_state, curr_action)\n",
        "\n",
        "                # Q-learning update\n",
        "                old_value = Q.get((curr_state, curr_action), 0.0)\n",
        "                next_actions = simulator.get_actions(next_state)\n",
        "                best_next = max(Q.get((next_state, a), 0.0) for a in next_actions)\n",
        "                Q[(curr_state, curr_action)] = reward + gamma * best_next\n",
        "\n",
        "                curr_state = next_state\n",
        "                if reward > 0 or curr_state == simulator.terminal_state:\n",
        "                    break  # stop rollout after first reward\n",
        "\n",
        "    # --- Extract optimal policy from Q-table ---\n",
        "    all_states = set(s for (s, a) in Q.keys())\n",
        "    for state in all_states:\n",
        "        actions = simulator.get_actions(state)\n",
        "        q_values = [Q.get((state, a), 0.0) for a in actions]\n",
        "        max_q = max(q_values)\n",
        "        best_actions = [a for a, qv in zip(actions, q_values) if qv == max_q]\n",
        "        best_action = random.choice(best_actions)\n",
        "        f.write(f\"{state}\\t{best_action}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google.colab.files.download(\"policy-optimal.tsv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5Uf_heNlk4uF",
        "outputId": "0a8f244d-17f3-45cf-e870-933023a9eb0e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b65aedca-aa3a-4b87-8dc6-eb40d45c1c46\", \"policy-optimal.tsv\", 13707)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BUoHvjUDkjf"
      },
      "source": [
        "Submit \"policy-optimal.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 6: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 7: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}